# =============================================================================
# Logstash Pipeline Configuration for Award Monitoring System
# =============================================================================
#
# How this works:
# 1. Input: Receives logs from your Spring Boot app via TCP on port 5000
# 2. Filter: Parses and enriches the log data
# 3. Output: Sends processed logs to Elasticsearch for storage and searching
# =============================================================================

input {
  # TCP input to receive logs from Spring Boot application
  # This corresponds to the LogstashTcpSocketAppender in logback-spring.xml
  tcp {
    port => 5000
    # Use json_lines codec because our app sends JSON-formatted logs
    codec => json_lines
    # Add type to distinguish different log sources
    type => "spring-boot-app"
  }
}

filter {
  # Only process logs that came from our TCP input
  if [type] == "spring-boot-app" {

    # Parse the timestamp from your logs
    # The logback encoder uses ISO8601 format with timezone
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target => "@timestamp"
    }

    # Add useful metadata
    mutate {
      # Add a field to identify this log stream
      add_field => {
        "log_source" => "spring-boot"
        "system" => "award-monitoring"
      }

      # Remove fields we don't need in Elasticsearch
      remove_field => ["port", "host"]
    }

    # If there's a stack trace, mark it as an error log
    if [stack_trace] {
      mutate {
        add_tag => ["error", "has_stacktrace"]
      }
    }

    # Tag logs by their level for easier filtering
    if [level] == "ERROR" {
      mutate {
        add_tag => ["error"]
      }
    } else if [level] == "WARN" {
      mutate {
        add_tag => ["warning"]
      }
    } else if [level] == "DEBUG" {
      mutate {
        add_tag => ["debug"]
      }
    }
  }
}

output {
  # Send all processed logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    # Create a new index per day for better management
    # This makes it easy to delete old logs by dropping entire indices
    index => "award-monitoring-logs-%{+YYYY.MM.dd}"

    # Define a template for how logs should be indexed
    # This ensures consistent field types across all log documents
    template_name => "award-monitoring-logs"
    template_overwrite => true
  }

  # Also output to stdout for debugging Logstash itself
  # Comment this out in production to reduce noise
  stdout {
    codec => rubydebug
  }
}
